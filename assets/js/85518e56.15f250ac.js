"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[84536],{28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>c});var a=t(96540);const r={},s=a.createContext(r);function i(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),a.createElement(s.Provider,{value:n},e.children)}},84445:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>m,frontMatter:()=>i,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"elements/balar/BalarInDepth","title":"Balar In Depth","description":"This doc provide some high level views on various aspects of balar.","source":"@site/../docs/elements/balar/BalarInDepth.md","sourceDirName":"elements/balar","slug":"/elements/balar/BalarInDepth","permalink":"/sst-docs/docs/elements/balar/BalarInDepth","draft":false,"unlisted":false,"editUrl":"https://github.com/sstsimulator/sst-docs/edit/master/docs/../docs/elements/balar/BalarInDepth.md","tags":[],"version":"current","lastUpdatedBy":"William-An","lastUpdatedAt":1742776902000,"frontMatter":{"title":"Balar In Depth"},"sidebar":"elements","previous":{"title":"Compiling RISCV + CUDA","permalink":"/sst-docs/docs/elements/balar/CompilingRISCVCUDA"},"next":{"title":"cacheTracer","permalink":"/sst-docs/docs/elements/cacheTracer/intro"}}');var r=t(74848),s=t(28453);const i={title:"Balar In Depth"},c=void 0,o={},l=[{value:"balar CUDA calls dispatch mechanism",id:"balar-cuda-calls-dispatch-mechanism",level:2},{value:"CUDA stream support",id:"cuda-stream-support",level:3},{value:"Why <code>pending_packets_per_stream</code> is needed",id:"why-pending_packets_per_stream-is-needed",level:4},{value:"<code>pending_packets_per_stream</code> insertion and removal",id:"pending_packets_per_stream-insertion-and-removal",level:4},{value:"Custom CUDA runtime library",id:"custom-cuda-runtime-library",level:2},{value:"Trace-driven mode component setup",id:"trace-driven-mode-component-setup",level:2},{value:"Direct-execution mode component setup",id:"direct-execution-mode-component-setup",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.p,{children:["This doc provide some high level views on various aspects of ",(0,r.jsx)(n.em,{children:"balar"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"balar-cuda-calls-dispatch-mechanism",children:"balar CUDA calls dispatch mechanism"}),"\n",(0,r.jsxs)(n.p,{children:["In ",(0,r.jsx)(n.em,{children:"balar"}),", every CUDA API call and return are represented by ",(0,r.jsx)(n.code,{children:"SST::BalarComponent::BalarCudaCallPacket_t"})," and ",(0,r.jsx)(n.code,{children:"SST::BalarComponent::BalarCudaCallReturnPacket_t"}),". These two structures contain necessary arguments for CUDA function calls and return values."]}),"\n",(0,r.jsxs)(n.p,{children:["Since ",(0,r.jsx)(n.em,{children:"balar"})," is a ",(0,r.jsx)(n.a,{href:"https://sst-simulator.org/sst-docs/docs/elements/memHierarchy/stdmem#mmio",children:"MMIO"})," (memory mapped IO) device, it receives CUDA call packets via incoming writes to its mapped address. Specifically, it follows the dispatch sequence as follow:"]}),"\n",(0,r.jsx)(n.mermaid,{value:"sequenceDiagram\n    autonumber\n    participant dmaEngine\n    participant balarMMIO\n    participant balarTestCPU\n    participant memory\n    balarTestCPU->>memory: Write CUDA API packet to<br/>scratch memory location\n    balarTestCPU->>balarMMIO: Write pointer to<br/>scratch memory location\n    balarMMIO->>dmaEngine: Issue a Read to<br/>retrieve the CUDA packet\n    dmaEngine->>balarMMIO: Return packet\n    balarMMIO->>balarMMIO: Call GPGPU-Sim functions\n    balarMMIO->>memory: Write CUDA return packet to pointer\n    balarMMIO->>balarTestCPU: Send response to the initial write "}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"BalarTestCPU"})," writes the pointer to the CUDA packet into ",(0,r.jsx)(n.em,{children:"balar"}),"'s MMIO address range, which ",(0,r.jsx)(n.em,{children:"balar"})," will use this to copy the actual packet content into simulator memory space."]})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:["With direct-execution, there are some differences with ",(0,r.jsx)(n.code,{children:"cudaMemcpy()"})," function calls. Specifically, ",(0,r.jsx)(n.em,{children:"balar"})," will need to copy data from SST memory system with ",(0,r.jsx)(n.code,{children:"cudaMemcpyHostToDevice"})," and copy data from simulator memory space into SST memory with ",(0,r.jsx)(n.code,{children:"cudaMemcpyDeviceToHost"})," using ",(0,r.jsx)(n.em,{children:"dmaEngine"}),"."]})}),"\n",(0,r.jsx)(n.h3,{id:"cuda-stream-support",children:"CUDA stream support"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"balar"})," supports CUDA streams through a lightweight stream manager (",(0,r.jsx)(n.code,{children:"pending_packets_per_stream"})," in ",(0,r.jsx)(n.code,{children:"balarMMIO.h"}),") that tracks every stream operation for every stream, similar to the ",(0,r.jsx)(n.code,{children:"stream_manager"})," class in GPGPU-Sim. In fact, ",(0,r.jsx)(n.code,{children:"pending_packets_per_stream"})," is just a mirror of ",(0,r.jsx)(n.code,{children:"stream_manager"}),". This is needed due to different designs between GPGPU-Sim and SST."]}),"\n",(0,r.jsxs)(n.h4,{id:"why-pending_packets_per_stream-is-needed",children:["Why ",(0,r.jsx)(n.code,{children:"pending_packets_per_stream"})," is needed"]}),"\n",(0,r.jsxs)(n.p,{children:["In GPGPU-Sim, the CUDA frontend is run on a separate thread than the backend simulation (i.e. the ",(0,r.jsx)(n.code,{children:"cycle()"})," function), meaning these two are ",(0,r.jsx)(n.strong,{children:"decoupled"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"The frontend handling does not need to sync with the performance simulation."}),"\n",(0,r.jsxs)(n.li,{children:["It can just push incoming CUDA stream operations to the ",(0,r.jsx)(n.code,{children:"stream_manager"})," class and let the backend consume the stream ops.","\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"In this case, when frontend deals with a blocking CUDA request, it will simply busy wait on the backend to drain the corresponding stream."}),"\n",(0,r.jsx)(n.li,{children:"This works as frontend and backend in original GPGPU-Sim can be run freely without synchronizations."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["But with SST, the frontend and backend are ",(0,r.jsx)(n.strong,{children:"coupled"})," together in balar, so extra callback functions and the ",(0,r.jsx)(n.code,{children:"pending_packets_per_stream"})," operation tracking are needed to prevent deadlocks due to the original decoupled design.\nWhich won't be solved by simply breaking the frontend and backend handling into different components within SST without any modification in the GPGPU-Sim side. This is because SST simulation will progress if at a given cycle, all the event handlings are done. However, for handling CUDA operation:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"The frontend will be handled in an event handler."}),"\n",(0,r.jsx)(n.li,{children:"The backend simulation will be in a clock tick function."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["This can easily lead to ",(0,r.jsx)(n.strong,{children:"deadlock"}),". Consider a scenario in ",(0,r.jsx)(n.em,{children:"balar"})," when a blocking CUDA request is sent to frontend handler with some active ops already in the stream of the incoming blocking request:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["In GPGPU-Sim, the stream manager will ",(0,r.jsx)(n.strong,{children:"busy waiting"})," on if the corresponding stream is empty, which is not in this case."]}),"\n",(0,r.jsxs)(n.li,{children:["However, the existing stream ops will only progress if the backend ",(0,r.jsx)(n.code,{children:"cycle()"})," in GPGPU-Sim is called."]}),"\n",(0,r.jsxs)(n.li,{children:["But the ",(0,r.jsx)(n.code,{children:"cycle()"})," will only be called in the next cycle of SST."]}),"\n",(0,r.jsx)(n.li,{children:"The next cycle will not come as the event handling in the frontend handler is not done."}),"\n",(0,r.jsxs)(n.li,{children:["Thus, a deadlock: ",(0,r.jsx)(n.code,{children:"frontend -> stream manager -> backend -> next cycle -> frontend"})]}),"\n"]}),"\n",(0,r.jsxs)(n.h4,{id:"pending_packets_per_stream-insertion-and-removal",children:[(0,r.jsx)(n.code,{children:"pending_packets_per_stream"})," insertion and removal"]}),"\n",(0,r.jsxs)(n.p,{children:["Upon a stream operation, CUDA call packet will be inserted to ",(0,r.jsx)(n.code,{children:"pending_packets_per_stream"})," at the corresponding stream. Some example patterns are listed below:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-CPP",children:"// Pattern 1: Memory Operations (Memcpy)\ncase CUDA_MEMCPY: {\n    // Create a copy of the packet\n    BalarCudaCallPacket_t * packet_copy = new BalarCudaCallPacket_t(*packet);\n    // Insert into default stream's pending packet queue\n    balar->pending_packets_per_stream.at(0).push(packet_copy);\n}\n\n// Pattern 2: Async Memory Operations\ncase CUDA_MEMCPY_ASYNC: {\n    // Create a copy of the packet\n    BalarCudaCallPacket_t * packet_copy = new BalarCudaCallPacket_t(*packet);\n    // Insert into specific stream's pending packet queue\n    balar->pending_packets_per_stream.at(packet_copy->cudaMemcpyAsync.stream).push(packet_copy);\n}\n\n// Pattern 3: Kernel Launch\ncase CUDA_LAUNCH: {\n    // Create a copy of the packet\n    BalarCudaCallPacket_t * packet_copy = new BalarCudaCallPacket_t(*packet);\n    // Insert into the configured stream's queue\n    balar->pending_packets_per_stream.at(config_stream).push(packet_copy);\n}\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Once the corresponding stream operation is completed (via ",(0,r.jsx)(n.code,{children:"SST_callback_event_done()"})," callback called from GPGPU-Sim), packet will be removed from ",(0,r.jsx)(n.code,{children:"pending_packets_per_stream"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-CPP",children:'void BalarMMIO::SST_callback_event_done(const char* event_name, cudaStream_t stream, \n    uint8_t* payload, size_t payload_size) {\n    \n    // Get reference to the stream\'s queue\n    auto& stream_queue = pending_packets_per_stream.at(stream);\n    // Get the head packet\n    BalarCudaCallPacket_t * head_packet = stream_queue.front();\n    \n    // Process different types of completion events\n    if (event_string == "memcpy_H2D_done") {\n        // Handle H2D completion\n        // ... process completion ...\n    } else if (event_string == "memcpy_D2H_done") {\n        // Handle D2H completion\n        // ... process completion ...\n    } else if (event_string == "Kernel_done") {\n        // Handle kernel completion\n        // ... process completion ...\n    }\n    \n    // Remove the completed packet from the queue\n    stream_queue.pop();\n}\n'})}),"\n",(0,r.jsx)(n.h2,{id:"custom-cuda-runtime-library",children:"Custom CUDA runtime library"}),"\n",(0,r.jsxs)(n.p,{children:["Located in ",(0,r.jsx)(n.code,{children:"src/sst/elements/balar/tests/vanadisLLVMRISCV"}),", the custom runtime lib ",(0,r.jsx)(n.code,{children:"cuda_runtime_api_vanadis.cc"})," will be linked with CUDA programs. For most CUDA APIs, it will create ",(0,r.jsx)(n.code,{children:"SST::BalarComponent::BalarCudaCallPacket_t"})," packets and send pointers to the packets to ",(0,r.jsx)(n.em,{children:"balar"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["For each CUDA call using ",(0,r.jsx)(n.code,{children:"makeCudaCall()"}),", ",(0,r.jsx)(n.em,{children:"balar"})," will first map its MMIO into ",(0,r.jsx)(n.em,{children:"vanadis"}),"'s virtual memory with memory fencing ops first. The actual ",(0,r.jsx)(n.code,{children:"mmap"})," call is performed via inline assembly code to avoid invalid accesses into ",(0,r.jsx)(n.em,{children:"balar"}),"'s MMIO address due to OoO execution. ",(0,r.jsx)(n.em,{children:"Balar"})," will unmap immediately after pointer is written for the same reason."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["For blocking CUDA calls, ",(0,r.jsx)(n.em,{children:"balar"})," will poll on the last CUDA API return status via ",(0,r.jsx)(n.code,{children:"readLastCudaStatus()"})," until the operation is completed."]}),"\n",(0,r.jsxs)(n.li,{children:["For non-blocking CUDA calls, ",(0,r.jsx)(n.em,{children:"balar"})," will return immediately."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"trace-driven-mode-component-setup",children:"Trace-driven mode component setup"}),"\n",(0,r.jsxs)(n.p,{children:["We provided a config script ",(0,r.jsx)(n.code,{children:"src/sst/elements/balar/tests/testBalar-testcpu.py"})," to run with trace information. The configuration graph roughly looks like this:"]}),"\n",(0,r.jsx)(n.mermaid,{value:"flowchart TD\n\tbalarTestCPU\n\tbalarMMIO\n\tdmaEngine\n\tmemory\n\trouter\n\tbalarTestCPU <--\x3e router\n\tbalarMMIO <--mmio_iface--\x3e router\n\tdmaEngine <--mem_iface--\x3e router\n\tdmaEngine <--mmio_iface--\x3e router\n\tmemory <--\x3e router"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"dmaEngine"})," has two memory interfaces. One for receiving commands (",(0,r.jsx)(n.code,{children:"mmio_iface"}),") and the other is used to access data (",(0,r.jsx)(n.code,{children:"mem_iface"}),")."]}),"\n",(0,r.jsx)(n.h2,{id:"direct-execution-mode-component-setup",children:"Direct-execution mode component setup"}),"\n",(0,r.jsxs)(n.p,{children:["For direct-execution with ",(0,r.jsx)(n.em,{children:"vanadis"}),", the config script is at ",(0,r.jsx)(n.code,{children:"src/sst/elements/balar/tests/testBalar-vanadis.py"}),", with configuration graph:"]}),"\n",(0,r.jsx)(n.mermaid,{value:'flowchart TD\n\tvanadisCore\n\tcoreTLB\n\tcoreCache\n\tvanadisOS\n\tosMMU\n\tbalar\n\tbalarTLB\n\tdmaEngine\n\tmemory\n\trouter\n\tcoreCacheBus{{coreCacheBus}}\n\n\tsubgraph " "\n\t\tdirection LR\n\t\tsubgraph VanadisCPU\n\t\t\tdirection TB\n\t\t\tvanadisCore <--\x3e coreTLB\n\t\t\tcoreTLB <--\x3e coreCacheBus\n\t\t\tcoreCacheBus <--\x3e coreCache\n\t\tend\n\t\t\n\t\tsubgraph OS\n\t\t\tdirection TB\n\t\t\tvanadisOS <--\x3e osMMU\n\t\tend\n\t\tbalarTLB <--\x3e coreCacheBus\n\t\tbalarTLB <--MMU::m_nicTlbLink--\x3e osMMU\n\t\tvanadisCore <--\x3e vanadisOS\n\t\tcoreTLB <--MMU::m_coreLinks--\x3e osMMU\n\t\tsubgraph Balar\n\t\t\tdirection TB\n\t\t\tdmaEngine <--\x3e balarTLB\n\t\t\tbalar\n\t\tend\n\tend\n\tbalar <--mmio interface--\x3e router\n\tcoreCache <--\x3e router\n\tdmaEngine <--mmio interface--\x3e router\n\trouter <--\x3e memory'}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsx)(n.p,{children:"Some details are omitted for simplicity."})}),"\n",(0,r.jsx)(n.admonition,{type:"note",children:(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"balar"})," needs a TLB as ",(0,r.jsx)(n.em,{children:"vanadis"})," works in virtual memory space. That part of the configuration script is based on the test example for ",(0,r.jsx)(n.a,{href:"/sst-docs/docs/elements/rdmaNic/intro",children:(0,r.jsx)(n.em,{children:"rdmaNic"})}),"."]})})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);